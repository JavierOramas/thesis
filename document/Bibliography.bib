@article{luhun1958,
  title={The Automatic Creation of Literature Abstracts},
  author={Luhn, H. P.},
  year={1958},
  journal={IBM Journal of Research and Development},
  url={https://psycnet.apa.org/doi/10.1147/rd.22.0159}
}

@inproceedings{mihalcea2004textrank,
  title={TextRank: Bringing Order into Texts},
  author={Mihalcea, R. and Tarau, P.},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2004},
  url={https://aclanthology.org/W04-3252.pdf}
}

@inproceedings{collins-etal-2017-supervised,
    title = "A Supervised Approach to Extractive Summarisation of Scientific Papers",
    author = "Collins, Ed  and
      Augenstein, Isabelle  and
      Riedel, Sebastian",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1021",
    doi = "10.18653/v1/K17-1021",
    pages = "195--205",
    abstract = "Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.",
}

@misc{acl,
  title = {ACL Anthology},
  url = {https://aclanthology.org/},
  note = {Accessed on December 4th 2023}
}

@misc{naacl,
  title = {NAACL: North American Chapter of the ACL (Association for Computational Linguistics)},
  url = {https://naacl.org/},
  note = {Accessed on December 4th 2023}
}

@misc{elicit,
  title = {Elicit},
  url = {https://elicit.com/},
  note = {Accessed on December 4th 2023}
}

@misc{scite,
  title = {Scite},
  url = {https://scite.ai/},
  note = {Accessed on December 4th 2023}
}

@article{knight2000statistics,
  title={Statistics-based summarization-step one: Sentence compression},
  author={Knight and Kevin and Marcu, Daniel},
  journal={AAAI/IAAI},
  volume={2000},
  pages={703--710},
  year={2000}
}

@article{liu2018,
  author    = {Peter J. Liu and
               Mohammad Saleh and
               Etienne Pot and
               Ben Goodrich and
               Ryan Sepassi and
               Lukasz Kaiser and
               Noam Shazeer},
  title     = {Generating Wikipedia by Summarizing Long Sequences},
  journal = {International Conference on Learning Representations},
  year      = {2018}
},

@article{fabbri2019multi-news,
  author    = {Alexander Fabbri and
               Irene Li and
               Tianwei She and
               Suyi Li and
               Dragomir Radev},
  title     = {Multi-news: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
  journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages     = {1074--1084},
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics}
},

@misc{arxivstats,
  title   = {arXiv Statistics},
  url     = {https://info.arxiv.org/help/stats/2021_by_area/index.html},
  note    = {Accessed on September 22, 2023}
},

@inproceedings{lu2020multixscience,
  author    = {Yao Lu and
               Yue Dong and
               Laurent Charlin},
  title     = {MultiXScience: A Large-Scale Dataset for Extreme Multidocument Summarization of Scientific Articles},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {8068--8074},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics}
}

@article{fewshot,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{BERT,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{RoBERTa,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@article{LexRank,
  author       = {G{\"{u}}nes Erkan and
                  Dragomir R. Radev},
  title        = {LexRank: Graph-based Lexical Centrality as Salience in Text Summarization},
  journal      = {CoRR},
  volume       = {abs/1109.2128},
  year         = {2011},
  url          = {http://arxiv.org/abs/1109.2128},
  eprinttype    = {arXiv},
  eprint       = {1109.2128},
  timestamp    = {Mon, 13 Aug 2018 16:48:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1109-2128.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{SeeLM17,
  author       = {Abigail See and
                  Peter J. Liu and
                  Christopher D. Manning},
  title        = {Get To The Point: Summarization with Pointer-Generator Networks},
  journal      = {CoRR},
  volume       = {abs/1704.04368},
  year         = {2017},
  url          = {http://arxiv.org/abs/1704.04368},
  eprinttype    = {arXiv},
  eprint       = {1704.04368},
  timestamp    = {Mon, 13 Aug 2018 16:46:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SeeLM17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rush2015neural,
  author       = {Alexander M. Rush and
                  Sumit Chopra and
                  Jason Weston},
  title        = {A Neural Attention Model for Abstractive Sentence Summarization},
  journal      = {CoRR},
  volume       = {abs/1509.00685},
  year         = {2015},
  url          = {http://arxiv.org/abs/1509.00685},
  eprinttype    = {arXiv},
  eprint       = {1509.00685},
  timestamp    = {Mon, 13 Aug 2018 16:46:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RushCW15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{PaulusXS17,
  author       = {Romain Paulus and
                  Caiming Xiong and
                  Richard Socher},
  title        = {A Deep Reinforced Model for Abstractive Summarization},
  journal      = {CoRR},
  volume       = {abs/1705.04304},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.04304},
  eprinttype    = {arXiv},
  eprint       = {1705.04304},
  timestamp    = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/PaulusXS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang,
  title={Integrating Extractive and Abstractive Models for Long Text Summarization},
  author={Shuai Wang and Xiang Zhao and Bo Li and Bin Ge and Daquan Tang},
  journal={2017 IEEE International Congress on Big Data (BigData Congress)},
  year={2017},
  pages={305-312},
  url={https://api.semanticscholar.org/CorpusID:21197619}
}

@article{cosum,
author = {Alguliyev, Rasim M. and Aliguliyev, Ramiz M. and Isazade, Nijat R. and Abdi, Asad and Idris, Norisma},
title = {COSUM: Text summarization based on clustering and optimization},
journal = {Expert Systems},
volume = {36},
number = {1},
pages = {e12340},
keywords = {adaptive differential evolution algorithm, content coverage, harmonic mean, information diversity, k-means, optimization model, sentence clustering, text summarization},
doi = {https://doi.org/10.1111/exsy.12340},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12340},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12340},
year = {2019}
}

@article{hybrid-llm,
title = {Hybrid multi-document summarization using pre-trained language models},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116292},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116292},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015979},
author = {Alireza Ghadimi and Hamid Beigy},
keywords = {Pre-trained language models, Extractive summarization, Abstractive summarization, Determinantal point process, Deep submodular network}
}

@misc{openai,
  author = {OpenAI},
  title = {{ChatGPT: OpenAI’s GPT-3-Based Language Model}},
  howpublished = {\url{https://www.openai.com/blog/chatgpt}},
  year = {2021},
  note = {Accessed: 12/16/2023}
}


@misc{basyal2023text,
      title={Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models}, 
      author={Lochan Basyal and Mihir Sanghvi},
      year={2023},
      eprint={2310.10449},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 
@misc{sia2020tired,
      title={Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!}, 
      author={Suzanna Sia and Ayush Dalmia and Sabrina J. Mielke},
      year={2020},
      eprint={2004.14914},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lda2003,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent Dirichlet Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {993–1022},
numpages = {30}
}

@article{neuraltm,
title = {A neural topic model with word vectors and entity vectors for short texts},
journal = {Information Processing and Management},
volume = {58},
number = {2},
pages = {102455},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102455},
url = {https://www.sciencedirect.com/science/article/pii/S030645732030947X},
author = {Xiaowei Zhao and Deqing Wang and Zhengyang Zhao and Wei Liu and Chenwei Lu and Fuzhen Zhuang},
keywords = {Topic model, Short text, Variational auto-encoder, Word embedding, Entity embedding},
abstract = {Traditional topic models are widely used for semantic discovery from long texts. However, they usually fail to mine high-quality topics from short texts (e.g. tweets) due to the sparsity of features and the lack of word co-occurrence patterns. In this paper, we propose a Variational Auto-Encoder Topic Model (VAETM for short) by combining word vector representation and entity vector representation to address the above limitations. Specifically, we first learn embedding representations of each word and each entity by employing a large-scale external corpora and a large and manually edited knowledge graph, respectively. Then we integrated the embedding representations into the variational auto-encoder framework and propose an unsupervised model named VAETM to infer the latent representation of topic distributions. To further boost VAETM, we propose an improved supervised VAETM (SVAETM for short) by considering label information in training set to supervise the inference of latent representation of topic distributions and the generation of topics. Last, we propose KL-divergence-based inference algorithms to infer approximate posterior distribution for our two models. Extensive experiments on three common short text datasets demonstrate our proposed VAETM and SVAETM outperform various kinds of state-of-the-art models in terms of perplexity, NPMI, and accuracy.}
}

@misc{bertopic,
      title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure}, 
      author={Maarten Grootendorst},
      year={2022},
      eprint={2203.05794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{tminemb,
    author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
    title = "{Topic Modeling in Embedding Spaces}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {439-453},
    year = {2020},
    month = {07},
    abstract = "{Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00325},
    url = {https://doi.org/10.1162/tacl\_a\_00325},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00325/1923074/tacl\_a\_00325.pdf},
}

@misc{liu2020survey,
      title={A Survey on Contextual Embeddings}, 
      author={Qi Liu and Matt J. Kusner and Phil Blunsom},
      year={2020},
      eprint={2003.07278},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{angelov2020top2vec,
      title={Top2Vec: Distributed Representations of Topics}, 
      author={Dimo Angelov},
      year={2020},
      eprint={2008.09470},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Huang_2020, 
  series={KDD '20},
   title={Embedding-based Retrieval in Facebook Search},
   url={http://dx.doi.org/10.1145/3394486.3403305},
   DOI={10.1145/3394486.3403305},
   booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
   year={2020},
   month=aug, 
   collection={KDD '20}
  }

@misc{reimers2019sentencebert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2022simcse,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2023mteb,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{leaderboard,
  url={https://huggingface.co/spaces/mteb/leaderboard},
  title={MTEB Leaderboard},
  note={accessed on 12/17/2023}
}

@misc{basyal2023text,
      title={Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models}, 
      author={Lochan Basyal and Mihir Sanghvi},
      year={2023},
      eprint={2310.10449},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llamapaper,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{gptj,
  author = {{Hugging Face}},
  title = {Transformers Documentation: GPT-J},
  url = {https://huggingface.co/docs/transformers/model_doc/gptj},
  note = {Accessed: 12/19/2023}
}

@online{llama,
  author = {{Hugging Face}},
  title = {Transformers Documentation: LLaMA},
  url = {https://huggingface.co/docs/transformers/main/en/model_doc/llama},
  note = {Accessed: 12/19/2023}
}

@online{falcon,
  author = {{Hugging Face}},
  title = {Transformers Documentation: Falcon},
  url = {https://huggingface.co/docs/transformers/main/en/model_doc/falcon},
  note = {Accessed: 12/19/2023}
}
@online{mpt,
  author = {{Hugging Face}},
  title = {ModelCard: MPT-7B},
  url = {https://huggingface.co/mosaicml/mpt-7b},
  note = {Accessed: 12/19/2023}
}

@online{metallama,
  author = {MetaAI},
  url = {https://ai.meta.com/llama/},
  note = {Accessed: 12/19/2023}
}

@online{meta,
  author = {MetaAI},
  url = {https://ai.meta.com/},
  note = {Accessed: 12/19/2023}
}

@misc{llamapaper2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

