@article{luhun1958,
  title={The Automatic Creation of Literature Abstracts},
  author={Luhn, H. P.},
  year={1958},
  journal={IBM Journal of Research and Development},
  url={https://psycnet.apa.org/doi/10.1147/rd.22.0159}
}

@inproceedings{mihalcea2004textrank,
  title={TextRank: Bringing Order into Texts},
  author={Mihalcea, R. and Tarau, P.},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2004},
  url={https://aclanthology.org/W04-3252.pdf}
}

@inproceedings{collins-etal-2017-supervised,
    title = "A Supervised Approach to Extractive Summarisation of Scientific Papers",
    author = "Collins, Ed  and
      Augenstein, Isabelle  and
      Riedel, Sebastian",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1021",
    doi = "10.18653/v1/K17-1021",
    pages = "195--205",
    abstract = "Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.",
}

@misc{acl,
  title = {ACL Anthology},
  url = {https://aclanthology.org/},
  note = {Accessed on December 4th 2023}
}

@misc{naacl,
  title = {NAACL: North American Chapter of the ACL (Association for Computational Linguistics)},
  url = {https://naacl.org/},
  note = {Accessed on December 4th 2023}
}

@misc{elicit,
  title = {Elicit},
  url = {https://elicit.com/},
  note = {Accessed on December 4th 2023}
}

@misc{scite,
  title = {Scite},
  url = {https://scite.ai/},
  note = {Accessed on December 4th 2023}
}

@article{knight2000statistics,
  title={Statistics-based summarization-step one: Sentence compression},
  author={Knight and Kevin and Marcu, Daniel},
  journal={AAAI/IAAI},
  volume={2000},
  pages={703--710},
  year={2000}
}

@article{liu2018,
  author    = {Peter J. Liu and
               Mohammad Saleh and
               Etienne Pot and
               Ben Goodrich and
               Ryan Sepassi and
               Lukasz Kaiser and
               Noam Shazeer},
  title     = {Generating Wikipedia by Summarizing Long Sequences},
  journal = {International Conference on Learning Representations},
  year      = {2018}
},

@article{fabbri2019multi-news,
  author    = {Alexander Fabbri and
               Irene Li and
               Tianwei She and
               Suyi Li and
               Dragomir Radev},
  title     = {Multi-news: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
  journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages     = {1074--1084},
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics}
},

@misc{arxivstats,
  title   = {arXiv Statistics},
  url     = {https://info.arxiv.org/help/stats/2021_by_area/index.html},
  note    = {Accessed on September 22, 2023}
},

@inproceedings{lu2020multixscience,
  author    = {Yao Lu and
               Yue Dong and
               Laurent Charlin},
  title     = {MultiXScience: A Large-Scale Dataset for Extreme Multidocument Summarization of Scientific Articles},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {8068--8074},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics}
}

@article{fewshot,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{BERT,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{RoBERTa,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@article{LexRank,
  author       = {G{\"{u}}nes Erkan and
                  Dragomir R. Radev},
  title        = {LexRank: Graph-based Lexical Centrality as Salience in Text Summarization},
  journal      = {CoRR},
  volume       = {abs/1109.2128},
  year         = {2011},
  url          = {http://arxiv.org/abs/1109.2128},
  eprinttype    = {arXiv},
  eprint       = {1109.2128},
  timestamp    = {Mon, 13 Aug 2018 16:48:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1109-2128.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{SeeLM17,
  author       = {Abigail See and
                  Peter J. Liu and
                  Christopher D. Manning},
  title        = {Get To The Point: Summarization with Pointer-Generator Networks},
  journal      = {CoRR},
  volume       = {abs/1704.04368},
  year         = {2017},
  url          = {http://arxiv.org/abs/1704.04368},
  eprinttype    = {arXiv},
  eprint       = {1704.04368},
  timestamp    = {Mon, 13 Aug 2018 16:46:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SeeLM17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rush2015neural,
  author       = {Alexander M. Rush and
                  Sumit Chopra and
                  Jason Weston},
  title        = {A Neural Attention Model for Abstractive Sentence Summarization},
  journal      = {CoRR},
  volume       = {abs/1509.00685},
  year         = {2015},
  url          = {http://arxiv.org/abs/1509.00685},
  eprinttype    = {arXiv},
  eprint       = {1509.00685},
  timestamp    = {Mon, 13 Aug 2018 16:46:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RushCW15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{PaulusXS17,
  author       = {Romain Paulus and
                  Caiming Xiong and
                  Richard Socher},
  title        = {A Deep Reinforced Model for Abstractive Summarization},
  journal      = {CoRR},
  volume       = {abs/1705.04304},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.04304},
  eprinttype    = {arXiv},
  eprint       = {1705.04304},
  timestamp    = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/PaulusXS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang,
  title={Integrating Extractive and Abstractive Models for Long Text Summarization},
  author={Shuai Wang and Xiang Zhao and Bo Li and Bin Ge and Daquan Tang},
  journal={2017 IEEE International Congress on Big Data (BigData Congress)},
  year={2017},
  pages={305-312},
  url={https://api.semanticscholar.org/CorpusID:21197619}
}

@article{cosum,
author = {Alguliyev, Rasim M. and Aliguliyev, Ramiz M. and Isazade, Nijat R. and Abdi, Asad and Idris, Norisma},
title = {COSUM: Text summarization based on clustering and optimization},
journal = {Expert Systems},
volume = {36},
number = {1},
pages = {e12340},
keywords = {adaptive differential evolution algorithm, content coverage, harmonic mean, information diversity, k-means, optimization model, sentence clustering, text summarization},
doi = {https://doi.org/10.1111/exsy.12340},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12340},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12340},
year = {2019}
}

@article{hybrid-llm,
title = {Hybrid multi-document summarization using pre-trained language models},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116292},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116292},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015979},
author = {Alireza Ghadimi and Hamid Beigy},
keywords = {Pre-trained language models, Extractive summarization, Abstractive summarization, Determinantal point process, Deep submodular network}
}

@misc{openai,
  author = {OpenAI},
  title = {{ChatGPT: OpenAI’s GPT-3-Based Language Model}},
  howpublished = {\url{https://www.openai.com/blog/chatgpt}},
  year = {2021},
  note = {Accessed: 12/16/2023}
}


@misc{basyal2023text,
      title={Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models}, 
      author={Lochan Basyal and Mihir Sanghvi},
      year={2023},
      eprint={2310.10449},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
} 
@misc{sia2020tired,
      title={Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!}, 
      author={Suzanna Sia and Ayush Dalmia and Sabrina J. Mielke},
      year={2020},
      eprint={2004.14914},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lda2003,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent Dirichlet Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {993–1022},
numpages = {30}
}

@article{neuraltm,
title = {A neural topic model with word vectors and entity vectors for short texts},
journal = {Information Processing & Management},
volume = {58},
number = {2},
pages = {102455},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102455},
url = {https://www.sciencedirect.com/science/article/pii/S030645732030947X},
author = {Xiaowei Zhao and Deqing Wang and Zhengyang Zhao and Wei Liu and Chenwei Lu and Fuzhen Zhuang},
keywords = {Topic model, Short text, Variational auto-encoder, Word embedding, Entity embedding},
abstract = {Traditional topic models are widely used for semantic discovery from long texts. However, they usually fail to mine high-quality topics from short texts (e.g. tweets) due to the sparsity of features and the lack of word co-occurrence patterns. In this paper, we propose a Variational Auto-Encoder Topic Model (VAETM for short) by combining word vector representation and entity vector representation to address the above limitations. Specifically, we first learn embedding representations of each word and each entity by employing a large-scale external corpora and a large and manually edited knowledge graph, respectively. Then we integrated the embedding representations into the variational auto-encoder framework and propose an unsupervised model named VAETM to infer the latent representation of topic distributions. To further boost VAETM, we propose an improved supervised VAETM (SVAETM for short) by considering label information in training set to supervise the inference of latent representation of topic distributions and the generation of topics. Last, we propose KL-divergence-based inference algorithms to infer approximate posterior distribution for our two models. Extensive experiments on three common short text datasets demonstrate our proposed VAETM and SVAETM outperform various kinds of state-of-the-art models in terms of perplexity, NPMI, and accuracy.}
}

@misc{bertopic,
      title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure}, 
      author={Maarten Grootendorst},
      year={2022},
      eprint={2203.05794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{tm-in-emb,
    author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
    title = "{Topic Modeling in Embedding Spaces}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {439-453},
    year = {2020},
    month = {07},
    abstract = "{Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00325},
    url = {https://doi.org/10.1162/tacl\_a\_00325},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00325/1923074/tacl\_a\_00325.pdf},
}