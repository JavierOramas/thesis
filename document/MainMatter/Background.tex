\chapter{Estado del Arte}\label{chapter:state-of-the-art}
    Este capítulo proporciona una panorámica del estado de las investigaciones relacionadas con la generación de resúmenes automáticos y las herramientas necesarias para la resolución de problemas específicos de los enfoques con los que se generan dichos resúmenes.

\section{Sistemas de generación automática de resúmenes utilizando enfoques híbridos}

Un enfoque híbrido combina tanto las aproximaciones extractivas como abstractivas en el proceso de resumen de texto. La arquitectura típica de un summarizador de texto híbrido se muestra en la figura {} y consta comúnmente de las siguientes fases:



\begin{enumerate}
    \item Preprocesamiento
    \item Extracción de oraciones (fase extractiva): Extraer las oraciones clave del texto de entrada.\cite[(Wang et al., 2017)]{Wang}
    \item Generación del resumen (fase abstractiva): Generar el resumen final a partir de la aplicación de métodos abstractivos a las oraciones previamente extraídas.
    \item Post procesamiento: Verificar que las oraciones generadas sean válidas y se ajusten al formato deseado.
\end{enumerate}


En la propuesta de (Rasim et al., 2019)\cite{cosum} se utiliza un modelo de dos etapas que combina técnicas de clustering y optimización. En la primera etapa, se aplica el método de k-means para clusterizar las oraciones y descubrir múltiples temas en el texto. En la segunda etapa, se implementa un modelo de optimización para la selección de oraciones destacadas de los clusters. Otro enfoque interesante es el de (Ghadimi \& Beigy, 2022)\cite{hybrid-llm} que inicialmente construye un resumen extractivo a partir de varios documentos de entrada y luego lo utiliza para generar el resumen abstractivo. La gestión de la información redundante, un problema global en el campo de la síntesis de múltiples documentos se aborda en la primera etapa, donde se utilizan similitudes basadas en BERT\cite{BERT} para calcular la redundancia de los documentos.

La incorporación de Grandes Modelos de Lenguaje (LLM) ha revolucionado el campo de procesamiento de lenguaje natural (NLP), abriendo nuevas posibilidades y elevando el rendimiento en diversas tareas, entre ellas, la sumarización de textos. En investigaciones como la realizada por (Basyal et al, 2023)\cite{basyal2023text} se observa el excepcional desempeño de los modelos de lenguaje, principalmente el modelo text-davinci-003 de OpenAI\cite{openai}. No obstante se aclara que los modelos con los que fue comparado son significativamente mas peque\~nos y se sugiere hacer uso de las versiones más complejas de los mismos.

\section{Topic Modeling}

    El análisis de texto ha experimentado avances significativos en las últimas décadas y entre las técnicas más destacadas se encuentra el Modelado de Temas (Topic Modeling). Esta metodología, anclada en la minería de texto y el procesamiento de lenguaje natural, ha emergido como una herramienta esencial para descubrir patrones subyacentes y estructuras temáticas en grandes conjuntos de documentos. El Modelado de Temas se adentra en la complejidad de los datos textuales, permitiendo la identificación automática y la organización de temas latentes presentes en un corpus\cite{lda2003}.

    El uso de modelos basados en la arquitectura encoder-decoder se ha usado tanto para la generación de algoritmos de Topic Modeling tanto para textos cortos\cite{neuraltm} como para textos más extensos\cite{tminemb}. En todos los casos ha mejorado los resultados de modelos más tradicionales y ampliamente usados como Latent Dirichlet Analysis (LDA)\cite{lda2003}   

    Angelov (2020)\cite{angelov2020top2vec} presenta un nuevo modelo que utiliza embeddings semánticos para identificar temas sin requerir parámetros predefinidos ni listas personalizadas. A diferencia de métodos tradicionales, este captura la semántica de palabras y documentos, demostrando resultados más informativos y representativos en experimentos.

    \subsection{Clustering}
    Utilizando una representación en vectores de la información a través de embeddings de texto (Sia et al. 2020)\cite{sia2020tired} presenta evaluaciones comparativas para la combinación de diferentes embeddings de palabras y algoritmos de agrupamiento y analiza el rendimiento bajo reducción de dimensionalidad con PCA\footnote{TODO}. En algunos casos ha tenido resultados tan sólidos como los modelos de temas clásicos, pero con menor tiempo de ejecución y complejidad computacional. 
    Por esta línea se encuentra también \emph{BERTopic}\cite{bertopic} que hace una primera representación de los documentos como text embeddings, realiza clustering en el espacio de los vectores generados y construye los títulos de los temas utilizando un sistema de representación categórica de los temas a través de una tabla TF-IDF\footnote{TODO}.

\section{Embeddings de texto}
    En la exploración de embeddings contextuales, según detalla Liu et al. (2020)\cite{liu2020survey}, el enfoque se centra en asignar representaciones a palabras basadas en su contexto, capturando sus diversos usos y codificando conocimientos transferibles entre idiomas. Su estudio profundiza en varios aspectos, incluidos los modelos existentes de embeddings contextuales, el preentrenamiento políglota entre idiomas y la aplicación de embeddings contextuales en tareas posteriores.

    Huang et al. (2020)\cite{Huang_2020} destacan las ventajas de la aplicación de estos al ámbito de los sistemas de búsqueda, especialmente en redes sociales como Facebook. Los autores presentan un marco de recuperación basado en embeddings (EBR) y un marco de embeddings unificado para la búsqueda personalizada, exponiendo sus experiencias y optimizaciones para mejoras integrales en el sistema. Su trabajo sobre EBR en la búsqueda de Facebook muestra ganancias significativas en métricas en experimentos online.

    Reimers et al. (2022)\cite{reimers2019sentencebert} abordan la sobrecarga computacional asociada con modelos como BERT y RoBERTa en tareas de regresión de pares de oraciones. Introducen Sentence-BERT (SBERT), una modificación de BERT que utiliza estructuras de redes siamesas y tripletas para derivar embeddings de oraciones semánticamente significativos. SBERT reduce significativamente el esfuerzo computacional requerido para encontrar pares de oraciones similares, manteniendo la precisión. Su evaluación demuestra un rendimiento superior en tareas de similitud textual semántica (\emph{STS}) y tareas de transferencia en comparación con otros métodos de embedding de oraciones de última generación.

    Gao et al. (2021)\cite{gao2022simcse} contribuyen al campo con SimCSE, presentando modelos de aprendizaje contrastivo no supervisado y supervisado que mejoran significativamente los embeddings de oraciones. Su enfoque, que utiliza dropout como un mínimo aumento de datos, logra resultados destacados en tareas de similitud textual semántica (\emph{STS}). El enfoque supervisado, que incorpora pares anotados, mejora aún más la correlación, destacando la efectividad del objetivo de aprendizaje contrastivo en la regularización y alineación de los embeddings preentrenados.

    Muenninghoff et al. (2023)\cite{muennighoff2023mteb} presentan el \emph{Massive Text Embedding Benchmark (MTEB)}, una iniciativa que abarca ocho tareas de embedding, 58 conjuntos de datos y 112 idiomas. A través de la evaluación de 33 modelos en \emph{MTEB}, se establece el benchmark más completo hasta la fecha. Los resultados revelan que ningún método de embedding de texto específico domina en todas las tareas, lo que sugiere que el campo aún no ha convergido hacia un método universal. MTEB se presenta como una solución de código abierto y un\cite[leaderboard público]{leaderboard}. Este enfoque transparente y accesible contribuye a un mejor entendimiento y selección de modelos de embedding de texto, fomentando el avance y la consolidación en este campo dinámico.

\section{Modelos de Lenguaje}
   
    La influencia transformadora de las arquitecturas de transformers, introducida en "Attention is All You Need" \cite{attention}, ha sido crucial para los grandes modelos de lenguaje(LLMs). Modelos como BERT\cite{BERT} avanzaron en la comprensión bidireccional del contexto, mientras que la serie GPT (Generative Pre-trained Transformer), especialmente GPT-3\cite{brown2020language}, con un modelo de 175 mil millones de parámetros y su sucesor GPT4\cite{openai2023gpt4}. Otros de los modelos a destacar son el presentados por MetaAI, LLaMA(Feb. 2023)\cite{llamapaper} y LLaMA2(Jul. 2023)\cite{llamapaper2} una colección de \emph{LLMs} preentrenados y ajustados finamente que varían en escala desde 7 mil millones hasta 70 mil millones de parámetros, mostrando resultados prometedores en comparación con otros modelos\cite{metallama}. 
    
    \subsection{Generación de resúmenes usando LLM}

    En el trabajo de (Basyal et al. 2023)\cite{basyal2023text} se exploran técnicas de resumen de texto utilizando \emph{LLMs}, incluyendo MPT-7b-instruct\cite{mpt}, falcon-7b-instruct\cite{falcon} y OpenAI ChatGPT \emph{text-davinci-003}\cite{brown2020language}. El modelo \emph{text-davinci-003} destaca como el más efectivo en diversas métricas de evaluación. Se realiza una comparación detallada en diferentes conjuntos de datos, como CNN, Daily Mail y XSum, ofreciendo valiosas percepciones sobre el rendimiento de los LLMs en entornos variados. 

    \subsection{Modelos de GPT4All}

        GPT4All es un ecosistema de software libre que ha publicado un grupo de \emph{LLMs} basados en la arquitectura \emph{transformers}\cite{attention}. Los modelos GPT4All son producto de la cuantización\footnote{Cuantizar es un proceso en el cual se reduce la precisión de los números en una red neuronal, disminuyendo así los requisitos de memoria y permitiendo que modelos grandes se ejecuten en dispositivos con recursos limitados. En este contexto, cuantizar facilita el uso de modelos GPT4All en computadoras con menos capacidad de hardware} de redes neuronales, un proceso que reduce los requisitos de VRAM\footnote{La memoria VRAM (Video Random Access Memory) es utilizada tratar los datos y las operaciones relacionadas con la tarjeta gráfica} para ejecutar un Decodificador Transformer de varios miles de millones de parámetros. A través de algoritmos de cuantización, estos modelos pueden ser adaptados para poder ser utilizados con un hardware moderado\cite{webgpt4all}.
        
        Entre los modelos soportados se encuentran\footnote{La cantidad de parámetros especificada se refiere a los modelos cuantizados disponibles en la web de GPT4All}:

        \begin{enumerate}
            \item Falcon (7 mil millones de parámetros)~\cite{falcon}
            \item LLaMA (7 - 13 mil millones de parámetros)~\cite{llama}
            \item MPT (7 mil millones de parámetros)~\cite{mpt}
            \item GPT-J (6 mil millones de parámetros)~\cite{gptj}
        \end{enumerate}

