\chapter{Propuesta}\label{chapter:proposal}
    Se ha decidido modelar el problema con un enfoque basado en () pasos.

    En una primera fase de "procesamiento de los documentos", donde se cargan los documentos que se desee procesar y se realiza un proceso de separación de forma tal que se puedan procesar como entidades más pequeñas (sea oraciones, párrafos u otro método de segmentación del texto) para poder tener la información en una forma más granular, lo que será primordial en el tercer paso.
    
    La segunda fase sería la conversión de la información previamente preprocesada en \emph{Embeddings de texto}. Se decidió utilizar el modelo "all-MiniML-L6-v2" que de acuerdo con las pruebas de MTEB[\cite{leaderboard}] tiene buenos resultados sin un alto requerimiento de tiempo ni de recursos computacionales. Estos embeddings son unos vectores de dimensión 384 que serán almacenados en una bases de datos vectorial para facilitar el acceso a estos y la realización de búsquedas por similaridad.
    Una vez obtenidos los \emph{embeddings}, serán procesados por un algoritmo de clustering que va a encargarse de agruparlos acorde a la similaridad, en la figura () se puede observar una representación en 2 dimensiones de un conjunto de documentos. Cada uno de los distintos clusters está definido por la cercaía de los vectores a los distintos centroides\footnote{Un centroide es el punto representativo de un grupo de datos, calculado como el promedio de las características de todos los puntos pertenecientes a ese grupo, y se utiliza como el centro o punto focal para asignar datos a un clúster específico en algoritmos de agrupamiento.}, estos centroides están representados también como vectores de la misma dimensionalidad que los embeddings generados, por tanto pueden ser utilizados para realizar una búsqueda en la base de datos vectorial que contiene los documentos, obteniéndose así los documentos más relevantes para cada tema.

    Los documentos recuperados tras realizar una búsqueda en la base de datos representan un resumen extractivo de cada uno de los temas detectados por el paso anterior. El proceso de extracción devuelve también una referencia al documento original que contenía la información de cada uno de los elementos recuperados por la base de datos. Con este resumen, se procede a utilizar un \emph{LLM} para convertir este resumen extractivo en una abstracción que sea más coherente y legible.

    Finalmente, los textos generados son organizados acorde a un formato de salida predefinido, generalmente una enumeración de los temas definidos (con un título sufgerido utilizando el mismo \emph{LLM} que generó el resumen) y posteriormente el resumen correspondiente a cada tema.
