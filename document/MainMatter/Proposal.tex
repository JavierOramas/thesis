\chapter{Propuesta}\label{chapter:proposal}
    Para la solución de los problemas planteados se propone definir un \emph{pipeline} que contará con 5 etapas principales, los detalles de cada etapa se definien a continuación.

    Primera fase: Se cargan los documentos y se realiza una segmentación de la información (en oraciones, párrafos u otro método de segmentación del texto) con el objetivo de representarla de manera granular. Esto será primordial para una fase posterior que se enfoca en la recuperación de la información.
    
    Segunda fase: comprende la conversión de la información previamente preprocesada en \emph{Embeddings de texto}. Para ello se decidió utilizar el modelo `all-MiniML-L6-v2' que, de acuerdo con las pruebas de \textbf{MTEB}[\cite{leaderboard}] obtiene buenos resultados sin altos requerimientos de tiempo y recursos computacionales. Estos \emph{embeddings} son unos vectores de dimensión 384 que serán almacenados en una base de datos vectorial para facilitar el acceso a estos y la realización de búsquedas por similaridad.

    Tercera fase: Una vez obtenidos los \emph{embeddings} se procesan  mediante un algoritmo de \emph{clustering} que se encarga de agruparlos atendiendo a la similaridad. Cada uno de los distintos clústers está definido por la cercanía de los vectores a los distintos centroides\footnote{Un centroide es el punto representativo de un grupo de datos, calculado como el promedio de las características de todos los puntos pertenecientes a ese grupo, y se utiliza como el centro o punto focal para asignar datos a un clúster específico en algoritmos de agrupamiento.}. Estos centroides están representados a su vez como vectores de la misma dimensionalidad que los \emph{embeddings} generados, por tanto; pueden ser utilizados para realizar una búsqueda en la base de datos vectorial que contiene los documentos. De esta manera se obtienen los documentos más relevantes para cada tema.

    Cuarta Fase: Los documentos recuperados tras realizar la búsqueda en la base de datos representan un resumen extractivo de cada uno de los temas detectados en el paso anterior. El proceso de extracción devuelve también una referencia al documento original que contenía la información de cada elemento recuperado por la base de datos. Con este resumen, se procede a utilizar un \textbf{LLM} para convertir este resumen extractivo en una abstracción que sea más coherente y legible.

    Quinta Fase: Los textos generados son organizados acorde a un formato de salida predefinido, generalmente una enumeración de los temas definidos (con un título sugerido utilizando el mismo \textbf{LLM} que generó el resumen) y posteriormente el resumen correspondiente a cada tema.
