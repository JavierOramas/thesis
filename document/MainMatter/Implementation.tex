\chapter{Detalles de Implementación y Experimentos}\label{chapter:implementation}
En este capítulo se aborda la implementación de cada uno de los componentes que conforman el \emph{pipeline}
descrito en la propuesta de solución.

\section{Carga de documentos}
La carga de documentos se realiza haciendo uso de la biblioteca \emph{langchain}[\cite{langchain}]
en particular el submódulo \emph{PyPDFLoader} del módulo \emph{document\_loaders}, no obstante, este puede ser sustituido por otro módulo siempre que la respuesta cuente con campos \emph{metadata} y \emph{page\_content}. En caso de sustituir completamente el módulo de carga de documentos se debe garantizar que el resultado de este primer paso sean dos listas, una con el texto de los documentos y otra con los metadatos de los mismos.
    \subsection{Segmentación del texto}
        Una de las limitaciones de los modelos de embedding y los \emph{LLMs} es la cantidad máxima de tokens\footnote{Un \emph{“token”} se refiere a una unidad individual de significado en un texto.} que puede procesar de una vez, es por esto que es necesario, en los casos de documentos muy extensos, realizar una segmentación del texto para hacer un procesamiento parcial e independiente de las partes.
        En la implementación de este documento se ofreecen tanto la segmentación por oraciones como la segmentación por párrafos, no obstante algunos párrafos pueden ser demasiado extensos y saturar la capacidad máxima del modelo.

Una vez cargados los documentos y segmentados, se asocia cada segmento de texto con los metadatos correspondientes.

\section{Transformación a vectores}
    \subsection{Modelo para la generación de \emph{embeddings}}
        En correspondencia con lo visto en el estudio del estado del arte, para la implementación de este \emph{pipeline} se utilizó un grupo de modelos de generación de \emph{embeddings} comprendidos entre las 15 primeras posiciones proporcionadas por \emph{MTEB}[\cite{leaderboard}] al momento de la revisión.

        Las dimensiones de los vectores generados por estos modelos de embedding varían entre los 1024, 768 y 384 elementos, además, la longitud de la secuencia que pueden procesar en todos los casos es de 512. Con este grupo de modelos de características diferentes se espera determinar si el desempeño de los modelos en esta tarea está directamente ligado a los resultados del modelo en \emph{MTEB} y evaluar cuán bien se desempeña un modelo más limitado en comparación con un modelo más extenso.

    Para la implementación del modelo de vectorización se creó una clase \emph{Embeddings} que va a actuar a modo de \emph{"wrapper"}\footnote{Un \emph{“wrapper”} es un programa o código que encapsula y facilita el uso de otros componentes del programa} en la implementación proporcionada se utiliza la biblioteca langchain[\cite{langchain}], particularmente el módulo de \emph{embeddings} de HuggingFace para cargar los modelos de \emph{embeddings}, se definen los métodos siguientes:
    \begin{enumerate}
        \item \emph{encode\_many}: Método que recibe una lista de documentos y retorna una lista con los vectores resultantes de procesar dicho documento
        \item \emph{encode}: Método que recibe una cadena de texto y retorna el vector resultante de procesar el documento 
        \item \emph{get\_model}: Método que retorna el modelo de embeddings que fue cargado. Esto es necesario para que tenga consistencia la búsqueda de similaridad de la base de datos
    \end{enumerate}

    Esta clase forma parte del pipeline, no obstante puede ser reemplazada por otra implementación que sea consecuente con el diseño descrito anteriormente.

    \subsection{Base de Datos Vectorial}
        \subsubsection{Selección del sistema de bases de datos}
            El sistema de bases de datos seleccionado para este proyecto es \emph{ChromaDB}[\cite{chromadb}]. Chroma DB es una base de datos vectorial que se destaca por su eficiencia en el almacenamiento y recuperación de vectores de \{embeddings}. Permite la creación de colecciones, el filtrado de texto y la consulta de documentos similares, además de tener una excelente integración con python.
        Las interacciones con la base de datos se hacen a través de una clase que actúa a modo de \emph{"wrapper"}, esta vez con la siguiente estructura:
        \begin{enumerate}
            \item \emph{create\_collection}: crea una nueva colección en la base de datos 
            \item \emph{get\_collection}: retorna una colección, las operaciones se realizan sobre las colecciones
            \item \emph{list\_collections}: retorna una lista con todas las colecciones existentes
            \item \emph{add} añade a la base de datos un nuevo documento, lo vectoriza utilizando el modelo de \emph{embeddings} definido, si no se define ninguno, utiliza por defecto el modelo `all-MiniLM-L6-v2' de \emph{SentenceTransformers}, recibe como entrada el texto del documento y los metadatos.
            \item \emph{add\_embedding\_to\_database}: añade un nuevo documento que ya haya pasado por el proceso de \emph{embedding},recibe como entrada el texto del documento, el vector que lo representa y los metadatos.
            \item \emph{add\_embeddings}: añade un grupo de documentos ya vectorizados a la base de datos, recibe como entrada el texto del documento, los vectores y los metadatos. 
            \item \emph{query}: realiza una búsqueda de documentos relevantes para una consulta, la entrada será el texto  de la consulta y retorna los documentos relevantes, con vectores, texto y metadatos, la consulta es vectorizada por el modelo de embeddings definido.
            \item \emph{query\_embeddings}: tiene un comportamineto similar a \emph{query} pero recibe en vez del texto a vectorizar, el vector que los representa; retorna los documentos relevantes (texto, vectores, metadatos)
        \end{enumerate}

        Todos los métodos de recuperación de información tienen un parámetro opcional \emph{top\_k} que define la cantidad de documentos a retornar.

        Los métodos de inserción comprenden un mecanísmo que revisa si existen ya en la base de datos otros documentos que puedan ser redundantes con el nuevo documento que se desea insertar. La similaridad máxima permitida entre dos vectores es 0.9, no obstante este valor es modificable para tener un mayor control sobre el criterio de similaridad, en correspondencia con el modelo de embeddings que se utilice.

        Este módulo, como todos los demás del \emph{pipeline}, es reemplazable por otros personalizados siempre y cuando sean consecuentes con la \emph{API}\footnote{Una API (Interfaz de Programación de Aplicaciones) es un conjunto de definiciones y protocolos que permite a diferentes aplicaciones comunicarse entre sí y compartir información y funcionalidades.} descrita.

\section{Detección de temas}
    El proceso de detección de los temas (\emph{Topic Modelling}) se realiza a través de un proceso de agrupamiento o \emph{clustering}\footnote{El clustering o análisis de agrupamiento es una técnica de aprendizaje no supervisado que agrupa un conjunto de datos en diferentes grupos o cl\'usteres} en el espacio de vectores que comprende los vectores que representan los documentos.

    \subsection{Algoritmo de \emph{Clustering}}
        Para este proceso se utiliza el algoritmo \emph{K-Means Clustering}, en particular la implementación que ofrece SKLearn[\cite{sklearn}] que además del algoritmo en sí, ofrece un método de detección automática de la cantidad óptima de cl\'usteres a través del método del codo[\cite{elbow}].
    
    Se implementó un \emph{wrapper} para este proceso, el cual comprende los siguientes métodos:
    \begin{enumerate}
        \item \emph{generate\_embeddings}: Método para procesar los documentos en caso de que no se proporcionen las versiones vectorizadas de estos, retorna la lista de vectores correspondiente a los \emph{embeddings} de los documentos.
        \item \emph{detect\_outliers}: Detecta los vectores que pueden ser considerados \emph{outliers}\footnote{En estadística, un outlier es un punto de datos que difiere significativamente de otras observaciones} para evitar que estos afecten la correcta posición de los centroides.
        \item \emph{detect\_optimal\_k}: Detecta la cantidad óptima de agrupar los datos acorde al método del codo, la cantidad de grupos, por defecto se encuentra en el rango entre 2 y 15, no obstante estos valores pueden ser modificados a través de los parámetros \emph{lim\_sub} y \emph{lim\_sup} de la función.
        \item \emph{topic\_words}: Retorna las palabras más repetidas en cada uno de los cl\'usteres; retira de la lista de palabras las que estan definidas en las \emph{stopwords} de \emph{nltk}\footnote{Módulo Natural Language Toolkit https://www.nltk.org}
        \item \emph{get\_topics}: Método principal que recibe los documentos, los procesa y extrae los \emph{outliers}. Al grupo de documentos sin \emph{outliers} les detecta el número óptimo de cl\'usteres; una vez detectado este número, realiza el proceso de \emph{clustering}, esta vez con todos los documentos; de los cl\'usteres obtenidos, extrae las palabras representativas y, utilizando un \emph{LLM}, genera una sugerencia de título a partir de estas palabras. Este método retorna el centroide, las palabras más frecuentes y el título sugerido para cada cluster detectado.
    \end{enumerate}

    Este proceso genera una agrupación de los vectores de forma tal que se detectan distintos temas con los cuales asociar la información contenida en los documentos. Los centroides detectados están representados por vectores de la misma dimensionalidad que los vectores generados en en proceso de \emph{embedding}; si se hiciera una representación en 2D del proceso descrito se obtendría una representación similar a la Figura 2 
    \begin{figure}
        \centering
        \includegraphics[scale = 0.7]{Figures/clustering.png}
        \caption*{Figura 2: Representación en 2D de un corpus donde se ideintifican claramente 3 temas}
    \end{figure}
    donde los puntos de colores simbolizan los documentos pertenecientes a cada tema y las cruces negras los centroides de cada uno de los temas.

    De esta manera si el centroide representa el punto que define la pertenencia de los documentos a un cluster que a su vez representa un tema, al obtenerse los documentos más relevantes con relación al vector que representa el centroide se estarán obteniendo los documentos más relevantes para el tema representado por dicho cluster, todo esto en correspondencia con la calidad de la representación vectorial del modelo de embeddings utilizado.
\section{Generación de los resúmenes}
    Una vez obtenidos los clusters se utilizan los centroides para recuperar los documentos más relevantes para cada tema. En este paso es importante tener en cuenta el método de segmentación de texto que se utilizó en el paso de carga de documentos. Este determinaría la cantidad de documentos a recuperar para, consecuentemente, no obtener una cantidad de documentos superior o inferior a la deseada, pues esto podría afectar la calidad de los resúmenes generados.
    
    Estos documentos recuperados, conforman un resumen extractivo preliminar, este resumen es entonces utilizado como parte del prompt de un \emph{LLM} para generar una abstracción del mismo. Efectivamente se obtiene una lista de los resúmenes de cada una de las secciones con las que se generará el estado del arte asociado a esos documentos, en correspondencia con la plantilla definida.
    Los documentos recuperados que son utilizados para generar el resumen de cada tema contienen en sus metadatos la información suficiente para ideintificar a qué archivo del corpus pertenecen, se añaden las referencias a estos documentos de origen utilizando esta infromación construyendo así las referencias bibliográficas del Estado del arte generado.


\section{Plantilla de Estado del Arte}
Se define la plantilla de Estado del arte como un documento que consiste en una enumeración de los temas descubiertos, 
para cada elemento se tendría:
\begin{enumerate}
    \item Título sugerido para el tema
    \item Texto con la información generada y las referencias a las citas bibliográficas
    \item Bloque con la bibliográfía a modo de citas de los documentos de los que se obtuvo la información
\end{enumerate}

Esta plantilla es más una definición del formato del documento que se genera con la información resultante de la ejecución del \emph{pipeline} en un corpus de documentos.

\section{Experimentación y validación}
Para el proceso de validación se utilizaron las m\'etricas (P, R y F1)\footnote{\textbf{P (precisión)}: Es una medida que se define como a la fracción de instancias recuperadas que son relevantes. \textbf{R (recobrado)}: M\'etrica que inidca cu\'antos de los documentos relevantes fueron identificados correctamente \textbf{F1}: El valor F se considera como una media armónica que combina los valores de la precisión y recobrado} reportadas por \emph{BERTScore}[\cite{bertscore}] al evaluar los resultados del modelo en los \emph{datasets} \emph{Mult-X-Science}[\cite{multixscience}] y \emph{WikiAsp}[\cite{wikiasp}], enfocados en la generación de textos a modo de trabajos relacionados y resúmenes enfocados en diferentes puntos de interés respectivamente.

La experimentación fue realizada en la paltaforma \emph{Google Colab} utilizando la configuración con GPU de 16GB \emph{Nvidia V100} dicha experimentación tomó aprox 13 y () horas en los \emph{datasets} descritos. En las tablas 3.1 y 3.2 se presentan los resultados preeliminares del desempeño en dichos \emph{datasets}, es importante recalcar la ausencia de \emph{datasets} que se ajusten en extención al problema.

\begin{table}[htb]
    \centering
    \label{tab:example}
    \begin{tabular}{*6c}
      \toprule
      \multicolumn{2}{c}{P}  & \multicolumn{2}{c}{R} & \multicolumn{2}{c}{F1} \\ \cmidrule(r){1-2} \cmidrule(r){3-4} \cmidrule(l){5-6}
      Mean & Median  &  Mean & Median &     Mean & Median   \\
                                                      
      \bottomrule
    \end{tabular}
    \caption{Resultados de la evaluación en Multi-X-Science}

  \end{table}

  \begin{table}[htb]
    \centering
    \label{tab:example}
    \begin{tabular}{*6c}
      \toprule
      \multicolumn{2}{c}{P}  & \multicolumn{2}{c}{R} & \multicolumn{2}{c}{F1} \\ \cmidrule(r){1-2} \cmidrule(r){3-4} \cmidrule(l){5-6}
      Mean & Median  &  Mean & Median &     Mean & Median   \\
                                                      
      \bottomrule
    \end{tabular}
    \caption{Resultados de la evaluación en WikiAsp}

  \end{table}